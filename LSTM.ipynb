{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElimuMichael/TensorFlow/blob/master/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "30S9Ug_q3JOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "4b08879b-03bd-4e1c-95d3-5e00938be520"
      },
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting torch (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 21kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x6238c000 @  0x7f1d8dc2f2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torch-1.0.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XRgIvuXzLOPU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba9e5465-67f5-495c-c4dd-03d58ede49f0"
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#@title\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "from google.colab import files, drive\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "drive_path = \"/content/drive/My Drive/\"\n",
        "file_path = drive_path + 'LSTM/data/anna.txt'"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2mdd161kI89d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BqtlsFnCP3uS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "75c5e002-ed89-40c4-e381-47796c5f42c3"
      },
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "with open(file_path, 'r') as data:\n",
        "    text = data.read()\n",
        "\n",
        "# Tokenize the words\n",
        "chars = tuple(set(text))\n",
        "\n",
        "# print(chars)\n",
        "int2chars = dict(enumerate(chars))\n",
        "# print(int2chars)\n",
        "chars2int = {chx: i for i, chx in int2chars.items()}\n",
        "# print(chars2int)\n",
        "\n",
        "# Encoded text\n",
        "encoded = np.array([chars2int[ch] for ch in text])\n",
        "print(encoded[:100])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[76  4 81 40 68 53 59 63 36 43 43 43 61 81 40 40 78 63 18 81 64  1 39  1\n",
            " 53 34 63 81 59 53 63 81 39 39 63 81 39  1 20 53  3 63 53 46 53 59 78 63\n",
            " 33 11  4 81 40 40 78 63 18 81 64  1 39 78 63  1 34 63 33 11  4 81 40 40\n",
            " 78 63  1 11 63  1 68 34 63 19 54 11 43 54 81 78 79 43 43 21 46 53 59 78\n",
            " 68  4  1 11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7P8uWi5XJjOe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "def one_hot_encode(arr, n_labels):\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=\"float32\")\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    return one_hot\n",
        "# Test the one_hot encode\n",
        "# print(one_hot_encode(np.array([[1, 2, 6]]), 7))\n",
        "\n",
        "# Generate training mini-batches\n",
        "# Discard all the values that don't fit into the sequence length window\n",
        "# Each window is batch_size X sequence_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EW86fdRAJucE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffb63bbc-02f9-438a-854e-40248331fea6"
      },
      "cell_type": "code",
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''\n",
        "    Create a generator that returns batches of size\n",
        "    batch_size x seq_length\n",
        "    '''\n",
        "    # total of the batch size\n",
        "    batch_size_total = batch_size* seq_length\n",
        "    \n",
        "    # Number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Characters making full batches\n",
        "    arr = arr[:n_batches*batch_size_total]\n",
        "    # Reshape the array into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "    # Iterate over the batches using the window of size seq_length\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # Extract the features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # print(x)\n",
        "        # Targets shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        # print(y)\n",
        "        # Generator by lstting the value of y equals the value of\n",
        "        # of x shifted by one\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            # Make the last value of y equal to the first value of x in the sequence\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y\n",
        "\n",
        "# batches = get_batches(encoded, 8, 50)\n",
        "# x, y = next(batches)\n",
        "# print('x\\n', x[:10, :10])\n",
        "# print('y\\n', y[:10, :10])\n",
        "\n",
        "# Define the model\n",
        "# Check GPU's availability\n",
        "train_on_GPU = torch.cuda.is_available()\n",
        "if train_on_GPU:\n",
        "    print('Training on GPU')\n",
        "else:\n",
        "    print('No GPU available, training on CPU; consider making n_epochs smaller.')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU available, training on CPU; consider making n_epochs smaller.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hj-8tZpsJ-9d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \"\"\"docstring for CharRNN\"\"\"\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.01):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # Obtain the output and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_GPU):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gUtnWpsHKHeC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9c875b26-21c4-4cb8-f037-0119730ac18c"
      },
      "cell_type": "code",
      "source": [
        "def train(net, data, epochs=10,batch_size=10, seq_length=50, lr=0.01, clip=5, val_frac=0.2, print_every=10):\n",
        "    # Allow for training and use of the drop out\n",
        "    net.train()\n",
        "\n",
        "    # Define the optimizer and the loss function    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_GPU):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_GPU):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            targets = targets.long()\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_GPU):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "\n",
        "                    targets = targets.long()\n",
        "\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "\n",
        "# Instantiate the model\n",
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)\n",
        "\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n",
            "Epoch: 1/20... Step: 10... Loss: 3.2148... Val Loss: 3.1818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NpkQqtmWKX0t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Making Predictions\n",
        "def predict(net, char, h=None, top_k=None):\n",
        "    '''\n",
        "    Given a character, predict the next character.\n",
        "    Returns the predicted character and the hidden state\n",
        "    '''\n",
        "    # Tensor inputs\n",
        "    x = np.array([[net.chars2int[char]]])\n",
        "    x = one_hot_encode(x, len(net.chars))\n",
        "    inputs = torch.from_numpy(x)\n",
        "\n",
        "    if train_on_GPU:\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    # Detachh hidden state from the history\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    # Get the model output\n",
        "    out, h = net(inputs, h)\n",
        "\n",
        "    # Obtain the character probabilities\n",
        "    p = F.softmax(out, dim=1).data\n",
        "\n",
        "    if train_on_GPU:\n",
        "        p = p.cpu() # Move the results to the CPU\n",
        "\n",
        "    # Get the top characters\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(net.chars))\n",
        "\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    # Select the likely next character with some level of randomness\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "    # Return the encoded value of the predicted character and hidden state\n",
        "    return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vsXSySjTKe4s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "    if train_on_GPU:\n",
        "        net.cuda()\n",
        "\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Pass in the previous character and get a new one\n",
        "    for i in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KuaaQMmBKmSJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))\n",
        "\n",
        "# Save the model checkpoint\n",
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {\n",
        "    'n_hidden': net.n_hidden,\n",
        "    'n_layers': net.n_layers,\n",
        "    'state_dict': net.state_dict,\n",
        "    'tokens': net.chars\n",
        "}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)\n",
        "\n",
        "# Loading Checkpoint\n",
        "with open('rnn_20_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "\n",
        "loaded = CharRNN(checkpoint['tokens'],n_hidden=checkpoint['hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "print(sample(net, 2000, prime='And Michael Said', top_k=5))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}